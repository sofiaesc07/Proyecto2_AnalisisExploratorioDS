{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, LSTM\n",
    "from gensim import corpora, models\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "entities_data = pd.read_csv('entities_train.csv', sep='\\t')\n",
    "relations_data = pd.read_csv('relations_train.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anaso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\anaso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Eliminar filas con valores nulos si es necesario\n",
    "entities_data = entities_data.dropna()\n",
    "# Eliminar caracteres especiales y convertir a minúsculas\n",
    "entities_data['mention'] = entities_data['mention'].apply(lambda x: x.lower())\n",
    "entities_data['mention'] = entities_data['mention'].apply(lambda x: ''.join([char for char in x if char not in string.punctuation]))\n",
    "\n",
    "# Tokenización y eliminación de palabras vacías (stopwords)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "entities_data['mention'] = entities_data['mention'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores nulos\n",
    "relations_data = relations_data.dropna()\n",
    "# Convertir la columna 'type' a minúsculas\n",
    "relations_data['type'] = relations_data['type'].str.lower()\n",
    "# Eliminar caracteres especiales y convertir a minúsculas en entity_1_id y entity_2_id\n",
    "relations_data['entity_1_id'] = relations_data['entity_1_id'].apply(lambda x: x.lower())\n",
    "relations_data['entity_2_id'] = relations_data['entity_2_id'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renombrar las columnas 'type'\n",
    "entities_data.rename(columns={'type': 'entity_type'}, inplace=True)\n",
    "relations_data.rename(columns={'type': 'relation_type'}, inplace=True)\n",
    "\n",
    "# Combinar los datos\n",
    "combined_data = entities_data.merge(relations_data, on='abstract_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de texto\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(combined_data['mention'])\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(combined_data['relation_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo 1: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3901/3901 [==============================] - 617s 158ms/step - loss: 1.1239 - accuracy: 0.5382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d280181010>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cnn = Sequential()\n",
    "model_cnn.add(Embedding(input_dim=X.shape[1], output_dim=32))\n",
    "model_cnn.add(Conv1D(32, 5, activation='tanh'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "model_cnn.add(Dense(y.max()+1, activation='softmax'))\n",
    "model_cnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_cnn.fit(X_train.toarray(), y_train, epochs=1, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo 2: Asignación de Dirichlet Latente (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "documents = []\n",
    "for doc in X:\n",
    "    words = [feature_names[i] for i in doc.indices]\n",
    "    documents.append(words)\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in documents]\n",
    "lda_model = models.LdaModel(corpus, num_topics=10, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo 3: RNN y LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 204/1951 [==>...........................] - ETA: 1:49:29 - loss: 1.2269 - accuracy: 0.5280"
     ]
    }
   ],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(input_dim=X.shape[1], output_dim=32)) \n",
    "model_rnn.add(LSTM(32))  \n",
    "model_rnn.add(Dense(y.max()+1, activation='softmax'))\n",
    "model_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_rnn.fit(X_train.toarray(), y_train, epochs=1, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la precisión de cada modelo y generar un gráfico de barras para visualizarla\n",
    "models = [model_cnn, lda_model, model_rnn]\n",
    "accuracies = []\n",
    "for model in models:\n",
    "    loss, accuracy = model.evaluate(X_test.toarray(), y_test)\n",
    "    accuracies.append(accuracy)\n",
    "for i, accuracy in enumerate(accuracies):\n",
    "    print(f\"Accuracy (Model {i+1}):\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(len(models)), accuracies)\n",
    "plt.xlabel('Modelo')\n",
    "plt.ylabel('Precisión')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión para el modelo CNN\n",
    "predictions_cnn = model_cnn.predict(X_test.toarray())\n",
    "cm_cnn = confusion_matrix(y_test.argmax(axis=1), predictions_cnn.argmax(axis=1))\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_cnn, annot=True)\n",
    "plt.title('Confusion Matrix for CNN')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matriz de confusión para el modelo RNN\n",
    "predictions_rnn = model_rnn.predict(X_test.toarray())\n",
    "cm_rnn = confusion_matrix(y_test.argmax(axis=1), predictions_rnn.argmax(axis=1))\n",
    "plt.figure(figsize=(10,7))\n",
    "sns.heatmap(cm_rnn, annot=True)\n",
    "plt.title('Confusion Matrix for RNN')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
